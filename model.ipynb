{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLOVE_Captioning_InceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlvz11Gh155B"
      },
      "source": [
        "Baseline model at https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgaJaBqmEgzr"
      },
      "source": [
        "#!pip uninstall keras\n",
        "!pip install keras==2.3.1\n",
        "#!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXFWN7UW9EMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f0c016-73d5-4624-f8da-854b354bfd58"
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqG5Wyg_Ju6c"
      },
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "\n",
        "!unzip Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_text.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtcmiRP4J7Vu"
      },
      "source": [
        "\n",
        "def extract_features(directory):\n",
        "\tmodel = InceptionV3(weights='imagenet')\n",
        "\tmodel = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\tprint(model.summary())\n",
        "\tfeatures = dict()\n",
        "\n",
        "\tfor name in listdir(directory):\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timg = load_img(filename, target_size=(299, 299))\n",
        "    # Convert PIL image to numpy array of 3-dimensions\n",
        "\t\tx = img_to_array(img)\n",
        "    # Add one more dimension\n",
        "\t\tx = np.expand_dims(x, axis=0)\n",
        "\t\t# preprocess the images using preprocess_input() from inception module\n",
        "\t\tx = preprocess_input(x)\n",
        "\n",
        "\t\tfeature = model.predict(x, verbose=0)\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\t#print('>%s' % name)\n",
        "\treturn features\n",
        "\n",
        "# extract features from all images\n",
        "directory = '/content/Flicker8k_Dataset'\n",
        "features = extract_features(directory)\n",
        "\n",
        "print('Extracted Features: %d' % len(features))\n",
        "\n",
        "dump(features, open('features.pkl', 'wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d95-RFDRvtkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0d7b9d-3fcb-493c-e7c2-077f8a63adf4"
      },
      "source": [
        "import string\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "\treturn mapping\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items(): # for each img\n",
        "\t\tfor i in range(len(desc_list)): # there are 5 captions\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\tdesc = desc.split() # tokenize\n",
        "\t\t\tdesc = [word.lower() for word in desc] # convert to lower case\n",
        "\t\t\tdesc = [w.translate(table) for w in desc] # remove punctuation from each token\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1] # remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()] # remove tokens with numbers in them\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
        "\n",
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build a list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "\n",
        "filename = '/content/Flickr8k.token.txt'\n",
        "doc = load_doc(filename) # load descriptions doc\n",
        "descriptions = load_descriptions(doc) # gets img ids and captions\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "clean_descriptions(descriptions)\n",
        "save_descriptions(descriptions, 'descriptions.txt')\n",
        "\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size after cleaning: %d' % len(vocabulary))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size after cleaning: 8763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHJIse_ayakZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4f192c-5549-457f-da7c-75fb9ddc7127"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from pickle import dump\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\tif len(line) < 1: # skip empty lines\n",
        "\t\t\tcontinue\n",
        "\t\tidentifier = line.split('.')[0] # get the image identifier\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\tdoc = load_doc(filename) # descriptions already saved loaded here\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t\n",
        "\t\tif image_id in dataset: # skip images not in the set\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset} # {id : features}\n",
        "\treturn features\n",
        "\n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# load training dataset (6K) just has the img ids\n",
        "filename = '/content/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description max Length: %d' % max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description max Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UofBf89IvWl8",
        "outputId": "003b411e-4532-40da-ef59-d66bd2ae0211"
      },
      "source": [
        "# download pretrained weights for embedding: GLOVE\n",
        "!wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip \n",
        "!unzip glove.6B.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 19:45:14--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-12-09 19:45:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-12-09 19:45:14--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.96MB/s    in 6m 34s  \n",
            "\n",
            "2020-12-09 19:51:48 (2.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zAOt5urvpcy",
        "outputId": "f260447d-2d9b-400c-c4a2-da412def49b7"
      },
      "source": [
        "EMBEDDING_DIM = 300 # to match glove 300d\n",
        "USE_GLOVE= True\n",
        "\n",
        "if USE_GLOVE:\n",
        "  GLOVE_file= 'glove.6B.300d.txt' #'glove.6B.300d.txt'\n",
        "  embeddings_index = {}\n",
        "  with open(GLOVE_file) as f:\n",
        "      for line in f:\n",
        "          word, coefs = line.split(maxsplit=1)\n",
        "          coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "          embeddings_index[word] = coefs\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "  f.close()\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('num_words: ', vocab_size)\n",
        "  embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  print('Total number of null word embeddings:', \n",
        "        np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "num_words:  7579\n",
            "Total number of null word embeddings: 763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbAp0_WPz5m9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675c2abe-3281-41be-ae84-f5b1452eb6e7"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU, Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each description for the image\n",
        "\tfor desc in desc_list:\n",
        "\t\t# encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t# split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "\t\t\t# encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# store\n",
        "\t\t\tX1.append(photo)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)\n",
        "\n",
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "\t\t\tyield [[in_img, in_seq], out_word]\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length): \n",
        " # feature extractor model\n",
        " # InceptionV3 last layer\n",
        " inputs1 = Input(shape=(2048,))\n",
        " fe1 = Dropout(0.125)(inputs1)\n",
        " fe2 = Dense(256, activation='relu')(fe1)\n",
        " # sequence model\n",
        " inputs2 = Input(shape=(max_length,)) # gets data from descriptions\n",
        " se1 = Embedding(vocab_size, EMBEDDING_DIM, \n",
        "                 weights=[embedding_matrix], \n",
        "                 trainable=True, mask_zero=True)(inputs2) \n",
        " se2 = Dropout(0.125)(se1) \n",
        " se3 = GRU(256)(se2)\n",
        "\n",
        " # decoder model \n",
        " decoder1 = add([fe2, se3]) # concatinates image + caption\n",
        " decoder2 = Dense(256, activation='relu')(decoder1)\n",
        " outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        " model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summarize model\n",
        " model.summary()\n",
        " plot_model(model, to_file='model.png', show_shapes=True)\n",
        " return model\n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "#model = load_model('model_13.h5')\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 10\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\tmodel.save('model_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           (None, 2048)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 34, 300)      2273700     input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 2048)         0           input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 34, 300)      0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 256)          524544      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_4 (GRU)                     (None, 256)          427776      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 256)          0           dense_8[0][0]                    \n",
            "                                                                 gru_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 256)          65792       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 7579)         1947803     dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,239,615\n",
            "Trainable params: 5,239,615\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 591s 98ms/step - loss: 4.2627\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 590s 98ms/step - loss: 3.4788\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 588s 98ms/step - loss: 3.2019\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 586s 98ms/step - loss: 3.0262\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 585s 97ms/step - loss: 2.9053\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 586s 98ms/step - loss: 2.8127\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 586s 98ms/step - loss: 2.7412\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 584s 97ms/step - loss: 2.6868\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 583s 97ms/step - loss: 2.6429\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 586s 98ms/step - loss: 2.6053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnXJ6_PY6kna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2f777f-ac30-4ca8-92fa-b1f8d2a7bf63"
      },
      "source": [
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed the generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over the whole length of the sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length, padding='post', truncating='post')\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# prepare tokenizer on train set\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = '/content/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = 34 #max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "\n",
        "# prepare test set\n",
        "\n",
        "# load test set\n",
        "filename = '/content/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "\n",
        "# load the model\n",
        "filename = '/content/model_13.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n",
            "BLEU-1: 0.536773\n",
            "BLEU-2: 0.290199\n",
            "BLEU-3: 0.198197\n",
            "BLEU-4: 0.088599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhGrIoyc6kt6"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed the generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over the whole length of the sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length, padding='post', truncating='post')\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text\n",
        "\n",
        "def extract_features(filename):\n",
        "\tmodel = InceptionV3( weights='imagenet')\n",
        "\tmodel = Model(inputs= model.input, outputs= model.layers[-2].output)\n",
        "\timg = load_img(filename, target_size=(299, 299))\n",
        "\tx = img_to_array(img)\n",
        "\tx = np.expand_dims(x, axis=0)\n",
        "\tx = preprocess_input(x)\n",
        "\tfeature = model.predict(x, verbose=0)\n",
        "\treturn feature\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from training)\n",
        "max_length = 34\n",
        "# load the model\n",
        "model = load_model('/content/model_5.h5')\n",
        "# load and prepare the photograph\n",
        "photo_name='/content/Flicker8k_Dataset/179009558_69be522c63.jpg'\n",
        "photo = extract_features(photo_name)\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)\n",
        "Image.open(photo_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}